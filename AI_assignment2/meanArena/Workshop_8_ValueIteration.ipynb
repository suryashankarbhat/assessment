{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAI Workshop 8\n",
    "<small>(Version 1.1)</small>\n",
    "\n",
    "Below there are four examples and one exercise to be completed by the given deadline (read the text).\n",
    "\n",
    "These all focus on various aspects of value iteration.\n",
    "\n",
    "---\n",
    "\n",
    "## EXAMPLE 1: Value Iteration in Excel\n",
    "\n",
    "For this first example, we'll need the spreadsheet from this week's workshop materials, the file \n",
    "\"value_iteration.xls\".\n",
    "\n",
    "As its name suggests, this implements the value iteration algorithm.\n",
    "\n",
    "Let's start with the tab \"Textbook\". This represents the problem as we saw it in the Lecture (which is \n",
    "exactly how it is presented in Russell and Norvig). The utility of the terminal states is fixed. \n",
    "(The terminal states are marked in green).\n",
    "\n",
    "Each group of cells in the spreadsheet represents an iteration of the model.\n",
    "\n",
    "The first group of cells contain the initial set of utilities. The utilities of all non-terminal states \n",
    "are set to zero.\n",
    "\n",
    "The second group of cells give the utilities of each state after the first application of the Bellman equation.\n",
    "The utilities used in this calculation are the ones in the initial set.\n",
    "Check the formulae in the cells to see which values are used.\n",
    "\n",
    "Each subsequent group of cells represents another iteration. I have marked each cell yellow when it reaches \n",
    "its final value (ie the same as the utility under the optimum policy).\n",
    "This is also the value when it stops changing.\n",
    "\n",
    "Play with the reward and the discount factor (gamma) to see how this alters the results. If you reduce gamma, \n",
    "you will need to add additional iterations to reach stability.\n",
    "\n",
    "---\n",
    "\n",
    "## EXAMPLE 2: Action selection in Excel\n",
    "\n",
    "Go back to the original values for reward (-0.04) and gamma (1). \n",
    "\n",
    "Now compute the optimum policy from the utilities after the last iteration. You can do this on paper, or in Excel.\n",
    "\n",
    "You can check your solution against the slides.\n",
    "\n",
    "Hint: The optimum policy is the one which includes the action in each state that maximises expected utility.\n",
    "    \n",
    "---\n",
    "\n",
    "## EXAMPLE 3: Solving a simple MDP using the MDP toolbox\n",
    "\n",
    "The MDP Toolbox is an implementation of some MDP algorithms in Python. You will need to install this using: \n",
    "\n",
    "pip install pymdptoolbox\n",
    "\n",
    "Documentation is at: https://pymdptoolbox.readthedocs.io/en/latest/index.html\n",
    "\n",
    "Let's start with a really simple problem. \n",
    "\n",
    "We have 4 states and two actions.\n",
    "\n",
    "There are two actions, 0 is \"Stay\" and 1 is \"Right\". 0 always succeeds and leaves the agent in the same state. 1 moves the agent right with probability 0.8, stays in place with probability 0.2.\n",
    "\n",
    "The states are 0, 1, 2, 3. 0 is left of 1, which is left of 2 and so on. (Thus the states are in a line which runs 0, 1, 2, 3 from left to right.\n",
    "\n",
    "State 3 has a reward of 1, and the cost of any action is -0.04.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values:\n",
      " (2.766226988084275, 3.7438891127976524, 4.857502678650809, 6.12579511)\n",
      "Policy:\n",
      " (1, 1, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "import mdptoolbox\n",
    "import numpy as np\n",
    "\n",
    "# The MDP Toolbox defines MDPs through a probability array and a reward array.\n",
    "\n",
    "# \n",
    "\n",
    "# The probability array has shape (A, S, S), where A are actions and S\n",
    "# are states. So A arrays, each S x S, ie for each action specify the\n",
    "# transitions probabilities of reaching the second state by applying\n",
    "# that action in the first state.\n",
    "\n",
    "# So, to implement the action model described above, we need:\n",
    "P1 = np.array([[[1, 0, 0, 0],\n",
    "                [0, 1, 0, 0],\n",
    "                [0, 0, 1, 0],\n",
    "                [0, 0, 0, 1]],\n",
    "               [[0.2, 0.8, 0,   0],\n",
    "                [0,   0.2, 0.8, 0],\n",
    "                [0,   0,   0.2, 0.8],\n",
    "                [0,   0,   0,   1]]])\n",
    "\n",
    "# The reward array has shape (S, A, A), so there is a set of S arrays,\n",
    "# one for each state, and each is a vector of all the actions --- each\n",
    "# is the reward of executing the relevant actions in the state (so\n",
    "# this is really modelling cost of the action).\n",
    "R1 = np.array([[-0.04, -0.04], [-0.04, -0.04], [-0.04, -0.04], [1, 1]])\n",
    "\n",
    "# The util.check() function checks that the reward and probability matrices \n",
    "# are well-formed, and match.\n",
    "# \n",
    "# Success is silent, failure provides somewhat useful error messages.\n",
    "mdptoolbox.util.check(P1, R1)\n",
    "# To run value iteration we create a value iteration object, and run it. Note that \n",
    "# discount value is 0.9\n",
    "vi1 = mdptoolbox.mdp.ValueIteration(P1, R1, 0.9)\n",
    "vi1.run()\n",
    "# We can then display the values (utilities) computed, and look at the policy:\n",
    "print('Values:\\n', vi1.V)\n",
    "print('Policy:\\n', vi1.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This says that the optimum policy is to go Right in every state until reaching state 3, then Stay. \n",
    "\n",
    "---\n",
    "\n",
    "## EXAMPLE 4: Now setup your own MDP\n",
    "\n",
    "Consider another simple MDP. This is another cutdown version of the problem we looked at in the lecture.\n",
    "The states are:\n",
    "\n",
    "2 3\n",
    "\n",
    "0 1\n",
    "\n",
    "So that 2 is Up from 0 and 1 is Right of 0, and so on. If you set up the actions so that 0 is Right, 1 is Left, 2 is Up and 3 is Down, then you will find it easy to compare what you did with my solution when I post it.\n",
    "\n",
    "The motion model is the same as in the lectures (0.8 probability of moving in the direction of the action, and 0.1 probability of moving in each of the directions perpendicular to that of the action).\n",
    "\n",
    "The reward for state 3 is 1, and the reward for state 1 is -1.\n",
    "\n",
    "What is the policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## EXERCISE\n",
    "\n",
    "The reward and probability matrices for the example from the slides are in the file setup.py which you can find \n",
    "on Blackboard with this week's workshop materials.\n",
    "\n",
    "Use these to create and solve an MDP using value iteration from the MDP Toolbox.\n",
    "\n",
    "How does the policy compare with the one from the lecture (which we replicated in the Spreadsheet)? \n",
    "\n",
    "Use the probability matrix and the values computed by value iteration to calculate the policy yourself, and \n",
    "check what you get against the policy provided by MDP Toolbox.\n",
    "\n",
    "Write a short document (PDF, max 1 page) or Jupyter Notebook file (preferred) describing your solution and send \n",
    "it to **sparsons@lincoln.ac.uk** with subject *AAI Workshop 8 - NAME SURNAME*. Please submit your work by the \n",
    "<u>13th December 2020</u>. **It will not be graded, but only used by the lecturer to check the progress of the class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

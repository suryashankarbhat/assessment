{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAI Workshop 9\n",
    "<small>(Version 1.2)</small>\n",
    "\n",
    "Below there are 6 examples and one exercise to be completed by the given deadline (read the text).\n",
    "\n",
    "These mainly focus on policy iteration.\n",
    "    \n",
    "---\n",
    "\n",
    "## EXAMPLE 1: Solving an MDP (again)\n",
    "\n",
    "Again we will use the MDP Toolbox, an implementation of some MDP algorithms in Python. If you did not do this last time,  will need to install MDP Toolbox using: \n",
    "\n",
    "pip install pymdptoolbox\n",
    "\n",
    "Documentation is at: https://pymdptoolbox.readthedocs.io/en/latest/index.html\n",
    "\n",
    "We'll start with the problem that was used in Example 4 in Workshop 8. (This is thus the solution to that Example that I promised)\n",
    "\n",
    "We have 4 states and four actions, and the problem is basically the top right corner of the example from the MDP we looked at in the slides and which is in the textbook.\n",
    "\n",
    "The actions are: 0 is Right, 1 is Left, 2 is Up and 3 is Down.\n",
    "    \n",
    "The motion model is the same as in the lectures (0.8 probability of moving in the direction of the action, and 0.1 probability of moving in each of the directions perpendicular to that of the action).\n",
    "\n",
    "The states are 0, 1, 2, 3, and they are arranged like this:\n",
    "    \n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "2 & 3\\\\\n",
    "0 & 1\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "So that 2 is Up from 0 and 1 is Right of 0, and so on. The cost of any action (in any state) is -0.04.\n",
    "\n",
    "The reward for state 3 is 1, and the reward for state 1 is -1, and the agent does not leave those states.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values:\n",
      " (88.23133912867952, -99.9949804281095, 97.54814303782808, 99.9949804281095)\n",
      "Policy:\n",
      " (1, 0, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "import mdptoolbox\n",
    "import numpy as np\n",
    "\n",
    "# Since the probability array is (A, S, S), there are 4 arrays. Each\n",
    "# is a 4 x 4 array:\n",
    "P2 = np.array([[[0.1, 0.8, 0.1, 0  ], # Right, State 0\n",
    "                [0,   1,   0,   0  ], # State 1 is absorbing\n",
    "                [0.1, 0,   0.1, 0.8],\n",
    "                [0,   0,   0,   1  ]],# State 3 is absorbing\n",
    "               [[0.9, 0,   0.1, 0  ], # Left\n",
    "                [0,   1,   0,   0  ],\n",
    "                [0.1, 0,   0.9, 0  ],\n",
    "                [0,   0,   0,   1  ]],\n",
    "               [[0.1, 0.1, 0.8, 0  ], # Up\n",
    "                [0,   1,   0,   0  ],\n",
    "                [0,   0,   0.9, 0.1],\n",
    "                [0,   0,   0,   1  ]],\n",
    "               [[0.9, 0.1, 0,   0  ], # Down\n",
    "                [0,   1,   0,   0  ],\n",
    "                [0.8, 0,   0.1, 0.1],\n",
    "                [0,   0,   0,   1  ]]])\n",
    "\n",
    "# The reward array has one set of values for each state. Each is the\n",
    "# value of all the actions. Here there are four actions, all with the\n",
    "# usual cost:\n",
    "R2 = np.array([[-0.04, -0.04, -0.04, -0.04],\n",
    "               [-1,    -1,    -1,    -1],\n",
    "               [-0.04, -0.04, -0.04, -0.04],\n",
    "               [1,      1,     1,     1]])\n",
    "\n",
    "mdptoolbox.util.check(P2, R2)\n",
    "vi2 = mdptoolbox.mdp.ValueIteration(P2, R2, 0.99)\n",
    "vi2.run()\n",
    "print('Values:\\n', vi2.V)\n",
    "print('Policy:\\n', vi2.policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This says that the optimum policy is to go Right in every state except State 0, and in State 0 go Left.\n",
    "\n",
    "We can basically ignore the actions for State 1 and State 3. Since they are terninal states, the action doesn't matter. As the motion model says, whatever action is picked, then agent stays in place and I strongly suspect that the code is always picking action 0 (whatever 0 is) in this situation.\n",
    "\n",
    "Right obviously makes sense for State 2 since that takes the agent to the goal state 3.\n",
    "\n",
    "But why is the policy to go Left in State 1 rather than Up?\n",
    "\n",
    "The short answer is that is because it is the action with the highest expected utility :-).\n",
    "\n",
    "The longer answer is that Left means that there is zero probability that the agent will end up in State 1 with its reward of -1, but will also, eventually, find its way to State 2, and hence to the positive reward of State 3.\n",
    "\n",
    "---\n",
    "\n",
    "## EXAMPLE 2: A different discount\n",
    "\n",
    "The choice of Left in State 0 is partly the result of the discount, which we have set to 0.99 (close to what we used in the lecture, but less than 1 to ensure convergence.)\n",
    "\n",
    "This means the agent doesn't lose much by putting off the positive reward from State 3.\n",
    "\n",
    "A lower value of the discount (which means rewards further away count for less) will force the agent to go Up in State 0.\n",
    "\n",
    "What value will do this?\n",
    "\n",
    "---\n",
    "\n",
    "## EXAMPLE 3: A different cost of actions\n",
    "\n",
    "The choice of Left in State 0 is also partly the result of the cost of action/reward for non-terminal states.\n",
    "\n",
    "The value of -0.04 is not a big price to pay for avoiding State 3.\n",
    "\n",
    "A higher action cost will force the agent to go Up in State 0. \n",
    "\n",
    "What value will do this?\n",
    "\n",
    "---\n",
    "\n",
    "## EXAMPLE 4: Finally policy iteration\n",
    "\n",
    "Although we have been looking at the policy, we go it through value iteration.\n",
    "\n",
    "Solving the same problem using policy iteration is easy with the MDP Toolbox:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values:\n",
      " (5.633171754225077, -10.000000000000002, 8.425258744923362, 10.000000000000002)\n",
      "Policy:\n",
      " (2, 0, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "pi2 = mdptoolbox.mdp.PolicyIteration(P2, R2, 0.9)\n",
    "pi2.run()\n",
    "print('Values:\\n', pi2.V)\n",
    "print('Policy:\\n', pi2.policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the methods disagree on the value while agreeing on the policy. This is typical, and is a feature of the termination/convergance conditions.\n",
    "\n",
    "---\n",
    "\n",
    "## EXAMPLE 5: Q-learning\n",
    "\n",
    "Solving a problem using reinforcement learning (well, the Q-learning kind of RL) is also easy using the MDP Toolbox:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values:\n",
      " (0.23990723950307208, -8.920270055220184, 3.8875765744931616, 9.999161625022545)\n",
      "Policy:\n",
      " (1, 1, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "rl2 = mdptoolbox.mdp.QLearning(P2, R2, 0.9)\n",
    "rl2.run()\n",
    "print('Values:\\n', rl2.V)\n",
    "print('Policy:\\n', rl2.policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this polRun this several times. What do you notice about the policy? Why do you think that this is the case?\n",
    "\n",
    "---\n",
    "\n",
    "## EXAMPLE 6: Now do it yourself\n",
    "\n",
    "Go back to the first example from the last workshop (the one with the states in a line).\n",
    "\n",
    "Solve it by policy iteration.\n",
    "\n",
    "How does the result compare with the result of value iteration?\n",
    "\n",
    "Look at the setVerbose() function and the time attribute of the MDP objects in MDPToolbox and use them to compare the number of iterations used by value iteration and policy iteration (using setVerbose()) and the CPU time used to come up with a solution (the time attribute).\n",
    "\n",
    "Now solve it using Q-learning.\n",
    "\n",
    "How does that result compare with the results of value iteration and policy iteration?\n",
    "\n",
    "---\n",
    "\n",
    "## EXAMPLE 6: A new version MDP of your own.\n",
    "\n",
    "Modify the MDP from Example 1 to make the \"bad place\" (the state with reward -1) State 2 rather than State 1, and then solve it using (a) policy iteration; and (b) Q-learning.\n",
    "\n",
    "Hint: you will have to modify the motion model as well as the rewards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## EXERCISE\n",
    "\n",
    "The reward and probability matrices for the example from the slides are in the file setup.py which you can find \n",
    "on Blackboard with last week's workshop material.\n",
    "\n",
    "Use these to create and solve an MDP using either policy iteration or Q-learning from the MDP Toolbox.\n",
    "\n",
    "How does the policy you get compare with the one from the lectures? \n",
    "\n",
    "Alter the discount until the policies agree.\n",
    "\n",
    "Now solve the problem using value iteration and compare the number of iterations, and the CPU time, used by the two methods.\n",
    "\n",
    "Write a short document (PDF, max 1 page) or Jupyter Notebook file (preferred) describing your solution and send \n",
    "it to **sparsons@lincoln.ac.uk** with subject *AAI Workshop 9 - NAME SURNAME*. Please submit your work by the \n",
    "<u>13th January 2022</u>. **It will not be graded, but only used by the lecturer to check the progress of the class**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
